1st Question

import numpy as np

# Sample data

study_hours = [2, 4, 6, 8, 10, 12]

exam_score = [45, 55, 65, 75, 85, 95]

# Study Hours statistics

print("Study Hours")

print("Mean:", np.mean(study_hours))

print("Median:", np.median(study_hours))

print("Standard Deviation:", np.std(study_hours))

# Exam Score statistics

print("\nExam Score")

print("Mean:", np.mean(exam_score))

print("Median:", np.median(exam_score))

print("Standard Deviation:", np.std(exam_score))

Study Hours

Mean: 7.0

Median: 7.0

Standard Deviation: 3.415650255319866

Exam Score

Mean: 70.0

Median: 70.0

Standard Deviation: 17.07825127659933

from scipy.stats import norm

import numpy as np

exam_score = [45, 55, 65, 75, 85, 95]

mean = np.mean(exam_score)

std = np.std(exam_score)

probability = 1 - norm.cdf(75, mean, std)

print("Probability of scoring more than 75:", probability)

Probability of scoring more than 75: 0.38484897189064493

import matplotlib.pyplot as plt

import seaborn as sns

sns.histplot(exam_score, bins=5, kde=True)

plt.xlabel("Exam Score")

plt.ylabel("Number of Students")

plt.title("Histogram of Exam Scores")

plt.show()
study_hours = [2, 4, 6, 8, 10, 12]

plt.scatter(study_hours, exam_score, color='red', alpha=1.0, s=100)

plt.title('Relationship Between Study Hours and Exam Marks')

plt.xlabel('Study Hours')

plt.ylabel('Exam Marks')

plt.grid(True)

plt.show()
2nd Question

import pandas as pd

data = {

"Student_ID": ["S1","S2","S3","S4","S5","S6","S7","S8"],

"Department": ["CSE","ECE","ME","CSE","EEE","CSE","ME","ECE"],

"Attendance": [85,92,70,88,75,95,68,80],

"Internal_Marks": [78,85,65,82,70,90,60,75]

}

df = pd.DataFrame(data)

import matplotlib.pyplot as plt

avg_marks = df.groupby("Department")["Internal_Marks"].mean()

avg_marks.plot(kind="bar")

plt.xlabel("Department")

plt.ylabel("Average Internal Marks")

plt.title("Average Internal Marks by Department")

plt.show()
plt.scatter(df["Attendance"], df["Internal_Marks"])

plt.xlabel("Attendance (%)")

plt.ylabel("Internal Marks")

plt.title("Attendance vs Internal Marks")

plt.show()
plt.boxplot(df["Internal_Marks"])

plt.ylabel("Internal Marks")

plt.title("Box Plot of Internal Marks")

plt.show()
import seaborn as sns

marks_data = df.pivot_table(

index="Student_ID",

values="Internal_Marks",

aggfunc="mean"

)

sns.heatmap(marks_data, annot=True, cmap="coolwarm")

plt.title("Heatmap of Student Performance")

plt.show()

3rd Question

import pandas as pd

data = {

"Study_Hours": [2,4,6,8,10,5,7,9],

"Attendance": [65,70,75,80,90,72,78,85],

"Internal_Marks": [45,55,65,78,90,60,70,85],

"Placed": [0,0,1,1,1,0,1,1]

}

df = pd.DataFrame(data)

import matplotlib.pyplot as plt

plt.scatter(df["Study_Hours"], df["Internal_Marks"])

plt.xlabel("Study Hours")

plt.ylabel("Internal Marks")

plt.show()
plt.scatter(df["Attendance"], df["Internal_Marks"])

plt.xlabel("Attendance (%)")

plt.ylabel("Internal Marks")

plt.show()
import pandas as pd

data = {

"Study_Hours": [2,4,6,8,10,5,7,9],

"Attendance": [65,70,75,80,90,72,78,85],

"Internal_Marks": [45,55,65,78,90,60,70,85],

"Placed": [0,0,1,1,1,0,1,1]

}

df = pd.DataFrame(data)

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error

X = df[["Study_Hours", "Attendance"]]

y = df["Internal_Marks"]

model = LinearRegression()

model.fit(X, y)

prediction = model.predict([[7, 80]])

print("Predicted Internal Marks:", prediction)

mse = mean_squared_error(y, model.predict(X))

print("MSE:", mse)
import matplotlib.pyplot as plt

plt.boxplot([df[df["Placed"]==0]["Internal_Marks"],

df[df["Placed"]==1]["Internal_Marks"]],

labels=["Not Placed","Placed"])

plt.ylabel("Internal Marks")

plt.show()
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score

X = df[["Study_Hours","Attendance","Internal_Marks"]]

y = df["Placed"]

log_model = LogisticRegression()

log_model.fit(X, y)

result = log_model.predict([[6, 78, 68]])

print("Placement Prediction:", result)
accuracy = accuracy_score(y, log_model.predict(X))

print("Accuracy:", accuracy)

4th Question

import pandas as pd

from sklearn.preprocessing import LabelEncoder

# Create the dataset

data = {

 'Contains_Offer': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No'],

 'Contains_Link': ['Yes', 'No', 'No', 'Yes', 'No', 'No'],

 'Contains_Attachment': ['Yes', 'Yes', 'No', 'No', 'Yes', 'No'],

 'Is_Spam': ['Spam', 'Not Spam', 'Spam', 'Not Spam', 'Spam', 'Not 

Spam']

}

df = pd.DataFrame(data)

# Initialize LabelEncoder

le = LabelEncoder()
df_encoded = df.copy()

for col in ['Contains_Offer', 'Contains_Link', 'Contains_Attachment', 

'Is_Spam']:

 df_encoded[col] = le.fit_transform(df[col])

# Display encoded data (Yes=1, No=0 | Spam=1, Not Spam=0)

print(df_encoded)
from sklearn.naive_bayes import BernoulliNB

import numpy as np

# Features (X) and Target (y)

X = df_encoded[['Contains_Offer', 'Contains_Link', 

'Contains_Attachment']]

y = df_encoded['Is_Spam']

# Initialize and train the model (BernoulliNB for binary data)

model = BernoulliNB()

model.fit(X, y)

print("Model trained successfully.")

Model trained successfully.

# Predict for: Contains_Offer=Yes(1), Contains_Link=No(0), 

Contains_Attachment=Yes(1)

new_email = [[1, 0, 1]]

prediction = model.predict(new_email)

result = "Spam" if prediction[0] == 1 else "Not Spam"

print(f"Predicted Class: {result}")
accuracy = model.score(X, y)

print(f"Accuracy: {accuracy * 100:.2f}%")
