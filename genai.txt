1st Question

import numpy as np
study_hours = [2, 4, 6, 8, 10, 12]
exam_score = [45, 55, 65, 75, 85, 95]
print("Study Hours")
print("Mean:", np.mean(study_hours))
print("Median:", np.median(study_hours))
print("Standard Deviation:", np.std(study_hours))
print("\nExam Score")
print("Mean:", np.mean(exam_score))
print("Median:", np.median(exam_score))
print("Standard Deviation:", np.std(exam_score))

from scipy.stats import norm
import numpy as np
exam_score = [45, 55, 65, 75, 85, 95]
mean = np.mean(exam_score)
std = np.std(exam_score)
probability = 1 - norm.cdf(75, mean, std)
print("Probability of scoring more than 75:", probability)

import matplotlib.pyplot as plt
import seaborn as sns
sns.histplot(exam_score, bins=5, kde=True)
plt.xlabel("Exam Score")
plt.ylabel("Number of Students")
plt.title("Histogram of Exam Scores")
plt.show()

study_hours = [2, 4, 6, 8, 10, 12]
plt.scatter(study_hours, exam_score, color='red', alpha=1.0, s=100)
plt.title('Relationship Between Study Hours and Exam Marks')
plt.xlabel('Study Hours')
plt.ylabel('Exam Marks')
plt.grid(True)
plt.show()

2nd Question

import pandas as pd
data = {
"Student_ID": ["S1","S2","S3","S4","S5","S6","S7","S8"],
"Department": ["CSE","ECE","ME","CSE","EEE","CSE","ME","ECE"],
"Attendance": [85,92,70,88,75,95,68,80],
"Internal_Marks": [78,85,65,82,70,90,60,75]
}
df = pd.DataFrame(data)

import matplotlib.pyplot as plt
avg_marks = df.groupby("Department")["Internal_Marks"].mean()
avg_marks.plot(kind="bar")
plt.xlabel("Department")
plt.ylabel("Average Internal Marks")
plt.title("Average Internal Marks by Department")
plt.show()

plt.scatter(df["Attendance"], df["Internal_Marks"])
plt.xlabel("Attendance (%)")
plt.ylabel("Internal Marks")
plt.title("Attendance vs Internal Marks")
plt.show()
plt.boxplot(df["Internal_Marks"])
plt.ylabel("Internal Marks")
plt.title("Box Plot of Internal Marks")
plt.show()

import seaborn as sns
marks_data = df.pivot_table(
index="Student_ID",
values="Internal_Marks",
aggfunc="mean"
)
sns.heatmap(marks_data, annot=True, cmap="coolwarm")
plt.title("Heatmap of Student Performance")
plt.show()

3rd Question

import pandas as pd
data = {
"Study_Hours": [2,4,6,8,10,5,7,9],
"Attendance": [65,70,75,80,90,72,78,85],
"Internal_Marks": [45,55,65,78,90,60,70,85],
"Placed": [0,0,1,1,1,0,1,1]
}
df = pd.DataFrame(data)

import matplotlib.pyplot as plt
plt.scatter(df["Study_Hours"], df["Internal_Marks"])
plt.xlabel("Study Hours")
plt.ylabel("Internal Marks")
plt.show()

plt.scatter(df["Attendance"], df["Internal_Marks"])
plt.xlabel("Attendance (%)")
plt.ylabel("Internal Marks")
plt.show()

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
X = df[["Study_Hours", "Attendance"]]
y = df["Internal_Marks"]
model = LinearRegression()
model.fit(X, y)
prediction = model.predict([[7, 80]])
print("Predicted Internal Marks:", prediction)
mse = mean_squared_error(y, model.predict(X))
print("MSE:", mse)

import matplotlib.pyplot as plt
plt.boxplot([df[df["Placed"]==0]["Internal_Marks"],
df[df["Placed"]==1]["Internal_Marks"]],
labels=["Not Placed","Placed"])
plt.ylabel("Internal Marks")
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
X = df[["Study_Hours","Attendance","Internal_Marks"]]
y = df["Placed"]
log_model = LogisticRegression()
log_model.fit(X, y)
result = log_model.predict([[6, 78, 68]])
print("Placement Prediction:", result)
accuracy = accuracy_score(y, log_model.predict(X))
print("Accuracy:", accuracy)

4th Question

import pandas as pd
from sklearn.preprocessing import LabelEncoder
data = {
 'Contains_Offer': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No'],
 'Contains_Link': ['Yes', 'No', 'No', 'Yes', 'No', 'No'],
 'Contains_Attachment': ['Yes', 'Yes', 'No', 'No', 'Yes', 'No'],
 'Is_Spam': ['Spam', 'Not Spam', 'Spam', 'Not Spam', 'Spam', 'Not 
Spam']
}
df = pd.DataFrame(data)
le = LabelEncoder()
df_encoded = df.copy()
for col in ['Contains_Offer', 'Contains_Link', 'Contains_Attachment', 
'Is_Spam']:
 df_encoded[col] = le.fit_transform(df[col])
print(df_encoded)

from sklearn.naive_bayes import BernoulliNB
import numpy as np
X = df_encoded[['Contains_Offer', 'Contains_Link', 
'Contains_Attachment']]
y = df_encoded['Is_Spam']
model = BernoulliNB()
model.fit(X, y)
print("Model trained successfully.")

Contains_Attachment=Yes(1)
new_email = [[1, 0, 1]]
prediction = model.predict(new_email)
result = "Spam" if prediction[0] == 1 else "Not Spam"
print(f"Predicted Class: {result}")
accuracy = model.score(X, y)
print(f"Accuracy: {accuracy * 100:.2f}%")

5th Question

import pandas as pd
data = {
"Math": [78,65,88,55,92,70],
"Physics": [72,60,85,58,90,68],
"Chemistry": [75,62,82,60,88,72],
"Programming": [85,70,90,65,95,75],
"Aptitude": [80,68,86,62,90,74]
}
students = ["S1","S2","S3","S4","S5","S6"]
df = pd.DataFrame(data, index=students)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)
print("Explained Variance Ratio:", pca.explained_variance_ratio_)


plt.scatter(pca_data[:,0], pca_data[:,1])
for i in range(len(students)):
 plt.text(pca_data[i,0], pca_data[i,1], students[i])
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA Scatter Plot of Students")
plt.show()

df['PC1'] = pca_data[:, 0]
df['PC2'] = pca_data[:, 1]
low_performers = df.sort_values(by='PC1', ascending=True)
low_performers

Performance Identification High Performance: S5 sits furthest to the right on the PC1 axis, 
indicating they have the highest scores across all subjects. Low Performance: S4 sits furthest to 
the left on the PC1 axis, indicating they have the lowest scores across subjects. Justification: The 
first principal component (PC1) accounts for the largest share of variance. Since all scores are 
positively correlated, high PC1 values correspond directly to high overall performance.
High Performance Students: Students S5 and S3 show overall high performance as they appear 
far from the origin in the positive direction of principal components. Low Performance Students: 
Students S4 and S2 show relatively low performance as they are closer to the origin or on the 
negative side of the PCA plot. Justification: In PCA, students farther from the origin represent 
higher overall performance across subjects.


